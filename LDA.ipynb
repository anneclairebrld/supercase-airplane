{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mancef\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mancef\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>verified</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_comment</th>\n",
       "      <th>type_of_traveler</th>\n",
       "      <th>start_point</th>\n",
       "      <th>end_point</th>\n",
       "      <th>date_of_flight</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>recommended</th>\n",
       "      <th>seat_comfort</th>\n",
       "      <th>cabin_staff_service</th>\n",
       "      <th>food_beverages</th>\n",
       "      <th>ground_service</th>\n",
       "      <th>value_for_money</th>\n",
       "      <th>flight_company</th>\n",
       "      <th>airplane_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-25</td>\n",
       "      <td>True</td>\n",
       "      <td>\"will never fly American again\"</td>\n",
       "      <td>Greensboro to Las Vegas via Charlotte. I wil...</td>\n",
       "      <td>Business</td>\n",
       "      <td>Greensboro</td>\n",
       "      <td>Las Vegas via Charlotte</td>\n",
       "      <td>January 2020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>american-airlines</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-25</td>\n",
       "      <td>False</td>\n",
       "      <td>\"just comfortable enough\"</td>\n",
       "      <td>Not Verified |  Tokyo to Dallas. I was dreadin...</td>\n",
       "      <td>Solo Leisure</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>January 2020</td>\n",
       "      <td>8.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>american-airlines</td>\n",
       "      <td>Boeing 777-200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>True</td>\n",
       "      <td>\"never fly with them again\"</td>\n",
       "      <td>Providence to Tucson via Chicago. American A...</td>\n",
       "      <td>Solo Leisure</td>\n",
       "      <td>Providence</td>\n",
       "      <td>Tucson via Chicago</td>\n",
       "      <td>January 2020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>american-airlines</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Horrible customer service\"</td>\n",
       "      <td>Not Verified |  Philadelphia to Boston. Horrib...</td>\n",
       "      <td>Business</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Boston</td>\n",
       "      <td>December 2019</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>american-airlines</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>True</td>\n",
       "      <td>\"missed my connecting flight\"</td>\n",
       "      <td>Houston to Dallas. I had departed from Ameri...</td>\n",
       "      <td>Solo Leisure</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>January 2020</td>\n",
       "      <td>4.0</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>american-airlines</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  verified                     review_title  \\\n",
       "0  2020-01-25      True  \"will never fly American again\"   \n",
       "1  2020-01-25     False        \"just comfortable enough\"   \n",
       "2  2020-01-24      True      \"never fly with them again\"   \n",
       "3  2020-01-24     False      \"Horrible customer service\"   \n",
       "4  2020-01-23      True    \"missed my connecting flight\"   \n",
       "\n",
       "                                      review_comment type_of_traveler  \\\n",
       "0    Greensboro to Las Vegas via Charlotte. I wil...         Business   \n",
       "1  Not Verified |  Tokyo to Dallas. I was dreadin...     Solo Leisure   \n",
       "2    Providence to Tucson via Chicago. American A...     Solo Leisure   \n",
       "3  Not Verified |  Philadelphia to Boston. Horrib...         Business   \n",
       "4    Houston to Dallas. I had departed from Ameri...     Solo Leisure   \n",
       "\n",
       "    start_point                end_point date_of_flight  overall_rating  \\\n",
       "0    Greensboro  Las Vegas via Charlotte   January 2020             1.0   \n",
       "1         Tokyo                   Dallas   January 2020             8.0   \n",
       "2    Providence       Tucson via Chicago   January 2020             1.0   \n",
       "3  Philadelphia                   Boston  December 2019             1.0   \n",
       "4       Houston                   Dallas   January 2020             4.0   \n",
       "\n",
       "  recommended  seat_comfort  cabin_staff_service  food_beverages  \\\n",
       "0          no           1.0                  1.0             1.0   \n",
       "1         yes           3.0                  4.0             4.0   \n",
       "2          no           1.0                  1.0             1.0   \n",
       "3          no           1.0                  1.0             1.0   \n",
       "4          no           2.0                  3.0             3.0   \n",
       "\n",
       "   ground_service  value_for_money     flight_company  airplane_model  \n",
       "0             1.0              1.0  american-airlines             NaN  \n",
       "1             4.0              4.0  american-airlines  Boeing 777-200  \n",
       "2             1.0              1.0  american-airlines             NaN  \n",
       "3             1.0              1.0  american-airlines             NaN  \n",
       "4             1.0              1.0  american-airlines             NaN  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Greensboro to Las Vegas via Charlotte. I wil...\n",
       "1    Not Verified |  Tokyo to Dallas. I was dreadin...\n",
       "2      Providence to Tucson via Chicago. American A...\n",
       "3    Not Verified |  Philadelphia to Boston. Horrib...\n",
       "4      Houston to Dallas. I had departed from Ameri...\n",
       "Name: review_comment, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Reviews = df[\"review_comment\"]\n",
    "Reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize the verbs - bring to the initial form\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))       \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break reviews to sentences\n",
    "review_sentences = []\n",
    "for review in Reviews:\n",
    "    for x in sent_tokenize(review):\n",
    "        review_sentences.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "processed_reviews = []\n",
    "for sentence in review_sentences:\n",
    "    processed_reviews.append(preprocess(lemmatize_stemming(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary\n",
    "dictionary = gensim.corpora.Dictionary(processed_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "bow_corpus = [dictionary.doc2bow(rev) for rev in processed_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TO DO -  Remove companies' names #####\n",
    "\n",
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 5,   # choose correct number \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.043*\"airlin\" + 0.029*\"servic\" + 0.023*\"travel\" + 0.021*\"fli\" + 0.021*\"custom\" + 0.020*\"experi\" + 0.019*\"emir\" + 0.016*\"time\" + 0.014*\"cost\" + 0.014*\"price\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.110*\"flight\" + 0.047*\"time\" + 0.037*\"hour\" + 0.023*\"delay\" + 0.020*\"arriv\" + 0.017*\"minut\" + 0.014*\"leav\" + 0.012*\"late\" + 0.012*\"book\" + 0.011*\"return\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.082*\"seat\" + 0.039*\"crew\" + 0.034*\"cabin\" + 0.033*\"flight\" + 0.019*\"extra\" + 0.017*\"good\" + 0.013*\"comfort\" + 0.013*\"friend\" + 0.010*\"aircraft\" + 0.010*\"plane\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.054*\"board\" + 0.046*\"check\" + 0.027*\"staff\" + 0.021*\"luggag\" + 0.018*\"airport\" + 0.016*\"pass\" + 0.014*\"tell\" + 0.013*\"print\" + 0.013*\"passeng\" + 0.012*\"help\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.129*\"ryanair\" + 0.029*\"fli\" + 0.028*\"dubai\" + 0.026*\"food\" + 0.026*\"stanst\" + 0.025*\"return\" + 0.023*\"drink\" + 0.022*\"london\" + 0.017*\"meal\" + 0.016*\"dublin\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "\n",
    "    \n",
    "# bow_corpus = [dictionary.doc2bow(rev) for rev in processed_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Sentiment score #######\n",
    "sentence_polarity = []\n",
    "sentence_sensitivity = []\n",
    "for sentence in review_sentences:\n",
    "    sentence_polarity.append(TextBlob(sentence).sentiment.polarity)\n",
    "    sentence_sensitivity.append(TextBlob(sentence).sentiment.subjectivity)\n",
    "#I have many zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, -0.2, 0.8, 0.25, 0.1875]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_polarity[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.45, 0.9, 0.75, 0.6375]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_sensitivity[0:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
